{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb828181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is divided into two main parts:\n",
    "\n",
    "# Part 1: Model Validation\n",
    "# In this section, the focus is on validating pre-trained models. \n",
    "# It involves assessing the performance of models that have already been trained, \n",
    "# using predefined parameters such as time index ranges for the validation dataset. \n",
    "# The goal is to evaluate the predictive accuracy of these models on test data.\n",
    "\n",
    "# Part 2: Model Training\n",
    "# This section is dedicated to training your own models. \n",
    "# It guides you through the process of setting up and configuring a new model, \n",
    "# including defining its parameters and architecture. \n",
    "# You will train the model using your dataset, allowing for experimentation \n",
    "# with different configurations to develop a model that best suits your data and prediction objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d7b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('./dataset/EMNN Decomposed Data.xlsx')\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "data[\"WTI\"] = data[\"WTI\"].astype(\"float64\")\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data[\"weekday\"] = data[\"weekday\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e151b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 256  # set this between 32 to 128\n",
    "\n",
    "\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# move dataloaders to device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76579de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82897a48",
   "metadata": {},
   "source": [
    "## Part 1: Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26b3ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5402dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for storing the trained model\n",
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=19-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=6-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=13-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v16.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v17.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v19.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v20.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v16.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v21.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=39.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v22.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v23.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v24.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=13-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v25.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v19.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v26.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v27.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v20.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=12-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v29.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v21.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=21-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v30.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v31.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v22.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v32.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v33.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v34.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v23.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v35.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v36.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v37.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v24.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v25.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v38.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v39.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v26.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v27.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba7c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c30b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"WTI_IMF1\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"WTI_IMF1\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9902639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "IMF1_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348431c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c9e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a40d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e94ff51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=7-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v40.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=33.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v41.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=33-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=40-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v51.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=32-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=41.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=25-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=5-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v52.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=39-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=24-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v53.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v37.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=40-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v38.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=21-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v39.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v21.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=40-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v54.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v55.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v56.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=12-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=39-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=17.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v7.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54193d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "805ea6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"WTI_IMF2\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"WTI_IMF2\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85fce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "IMF2_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d4b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea0586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d7012be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v62.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v62.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=24-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=11-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=24-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=24-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v62.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v62.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v67.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v62.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v28.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v3.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "564bec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ae6a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"WTI_Res\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"WTI_Res\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43c3abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "Res_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b8c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7d8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631523ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2777470",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [82.83, 84.36, 85.89, 83.7, 82.87, 87.67, 86.65, 86.66, 88.35, 89.35, 89.12, 85.49, 84.58, 86.07, 83.8, 86.04, 83.03, 81.64, 81.05, 83.04, 81.19, 81.54, 77.96, 75.85, 76.34, 77.6, 78.86, 78.9, 77.15, 73.5, 76.47, 78.1, 78.35, 76.8, 75.815, 74.83, 74.46, 76.09, 77.56, 75.66, 73.7, 72.73, 71.95, 68.98, 69.0, 70.87, 70.95, 68.27, 69.09, 71.21, 71.05, 72.16, 73.23, 73.87, 73.59, 73.29, 75.84, 74.31, 72.02, 71.89, 70.62, 72.97, 72.38, 74.0, 71.06, 72.43, 71.57, 72.15, 72.94, 72.785, 72.63, 72.79, 74.32, 73.69, 75.26, 74.72, 75.48, 77.91, 78.45, 77.25, 78.3, 76.28, 74.36, 72.72, 73.21, 73.83, 74.26, 76.67, 77.26, 77.34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97eb8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7718856141832139\n",
      "RMSE: 1.2555244300722526\n",
      "MAPE: 0.009821374345646963\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine IMF1, IMF2, and residual components to get the final forecast\n",
    "final = [sum(x) for x in zip(IMF1_fore, IMF2_fore, Res_fore)]\n",
    "# Adjust the elements in the 'final' list to balance the previous increments of 10 each to IMF1 and IMF2, \n",
    "# effectively removing the added 20 units per element.\n",
    "final = [x - 20 for x in final]\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE)\n",
    "mae = np.mean(np.abs(np.array(true_values) - np.array(final)))\n",
    "rmse = np.sqrt(np.mean(np.square(np.array(true_values) - np.array(final))))\n",
    "mape = np.mean(np.abs(np.array(true_values) - np.array(final)) / np.array(true_values))\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be7357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761e077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d8b1bc",
   "metadata": {},
   "source": [
    "## Part 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training the Temporal Fusion Transformer (TFT) model, \n",
    "# it is beneficial to employ the Tree-structured Parzen Estimator (TPE) method for hyperparameter optimization.\n",
    "# This step aids in selecting the optimal hyperparameters for the TFT model. \n",
    "# However, the TPE method can be computationally intensive and involves randomness, \n",
    "# hence it is optional. We have already incorporated the optimal hyperparameters\n",
    "# obtained from running this method into the corresponding TFT model.\n",
    "# If you wish to perform this optimization and discover your own optimal hyperparameters,\n",
    "# you may run the following code. Note that this process may significantly impact computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_prediction_length = 1\n",
    "# max_encoder_length = 30\n",
    "\n",
    "# training = TimeSeriesDataSet(\n",
    "#     data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "#     time_idx=\"time_idx\",\n",
    "#     target=\"WTI_IMF1\",\n",
    "#     min_encoder_length=max_encoder_length // 2, \n",
    "#     max_encoder_length=max_encoder_length,\n",
    "#     min_prediction_length=1,\n",
    "#     max_prediction_length=max_prediction_length,\n",
    "#     time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "#     time_varying_known_reals=[\"time_idx\"],\n",
    "#     time_varying_unknown_categoricals=[],\n",
    "#     time_varying_unknown_reals=[\n",
    "#         \"WTI_IMF1\",\n",
    "        \n",
    "#     ],\n",
    "#     group_ids=['destination'],\n",
    "#     target_normalizer=GroupNormalizer(\n",
    "#         groups=['destination'], transformation=\"softplus\"),\n",
    "#     add_relative_time_idx=True,\n",
    "#     add_target_scales=True,\n",
    "#     add_encoder_length=True,\n",
    "#     allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    "# )\n",
    "# validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "\n",
    "# # create dataloaders for model\n",
    "# batch_size = 256  # set this between 32 to 128\n",
    "\n",
    "\n",
    "# # check if GPU is available\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "# # move dataloaders to device\n",
    "\n",
    "\n",
    "# train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "# val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10 , num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c232620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# # create study\n",
    "# study = optimize_hyperparameters(\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     model_path=\"optuna_test\",\n",
    "#     n_trials=50,\n",
    "#     max_epochs=50,\n",
    "#     gradient_clip_val_range=(0.01, 1.0),\n",
    "#     hidden_size_range=(8, 128),\n",
    "#     hidden_continuous_size_range=(8, 128),\n",
    "#     attention_head_size_range=(1, 4),\n",
    "#     learning_rate_range=(0.001, 0.1),\n",
    "#     dropout_range=(0.1, 0.3),\n",
    "#     trainer_kwargs=dict(limit_train_batches=30),\n",
    "#     reduce_on_plateau_patience=4,\n",
    "#     use_learning_rate_finder=False\n",
    "# )\n",
    "\n",
    "\n",
    "# # save study results - also we can resume tuning at a later point in time\n",
    "# with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "#     pickle.dump(study, fout)\n",
    "\n",
    "# # show best hyperparameters\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec3f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6f228a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 1\n",
    "\n",
    "# Define the time index range for the validation dataset\n",
    "start_prediction_idx = 713 # Starting index for prediction\n",
    "end_prediction_idx = 802 # Ending index for prediction\n",
    "\n",
    "\n",
    "# Set the prediction length to 1, indicating that the model predicts 1 steps ahead\n",
    "prediction_length = 1\n",
    "\n",
    "# Note: The test set comprises 90 time points. The prediction_length of 1 means that \n",
    "# the models trained in this session are designed to forecast 1 steps ahead. The \n",
    "# start and end indices (713 to 802) specify the time points covered by the \n",
    "# model's predictions. These indices also determine the number of models to be trained, \n",
    "# calculated as: (end_prediction_idx - start_prediction_idx + 1) / prediction_length.\n",
    "# For start_prediction_idx = 713 and end_prediction_idx = 802 with prediction_length = 1,\n",
    "# the total number of models trained in the loop is 90.\n",
    "\n",
    "max_encoder_length = 30 \n",
    "\n",
    "# Initialize an empty list to store all prediction values\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize an empty list to store the best models for each prediction\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b758d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: The models are resource-intensive, requiring a high-performance GPU. Training \n",
    "# too many models in a single loop may lead to exhaustion of computer resources, causing \n",
    "# the program to crash. In such cases, you may need to restart the program or adjust the \n",
    "# start and end indices to reduce the number of models trained in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40532186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over prediction indices to generate training and validation sets dynamically\n",
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    # Define the training dataset, filtering data up to the current prediction index\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1], \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"WTI_IMF1\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"WTI_IMF1\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "    \n",
    "    # Copy validation data set based on prediction length and current index\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    \n",
    "    # Setup model checkpointing\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,  \n",
    "        filename=\"best_model_{epoch}\", \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "    \n",
    "    # Setup early stopping to prevent overfitting\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    \n",
    "    # Initialize logging for learning rate and training process\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "    \n",
    "    # Initialize trainer with specified parameters and callbacks\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1259662302414041,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Initialize the TFT model with specified hyperparameters\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.006878237268828405,\n",
    "        hidden_size=18,\n",
    "        attention_head_size=3,\n",
    "        dropout=0.2610076941913576,\n",
    "        hidden_continuous_size=9,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "    \n",
    "    # Log and store the path of the best performing model\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "    # Perform predictions using the best model\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c46015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a85d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802 \n",
    "\n",
    "prediction_length = 1\n",
    "max_encoder_length = 30 \n",
    "\n",
    "all_predictions = []\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4852465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1], \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"WTI_IMF2\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"WTI_IMF2\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    \n",
    "    # configure network and trainer\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,  \n",
    "        filename=\"best_model_{epoch}\",  \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.7808891799998493,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.017684809018645165,\n",
    "        hidden_size=121,\n",
    "        attention_head_size=1,\n",
    "        dropout=0.2930918827522154,\n",
    "        hidden_continuous_size=106,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bed2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ca53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c41c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1761f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802 \n",
    "\n",
    "prediction_length = 1\n",
    "max_encoder_length = 30 \n",
    "\n",
    "all_predictions = []\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26876501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1],  \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"WTI_Res\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"WTI_Res\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    \n",
    "    # configure network and trainer\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,  \n",
    "        filename=\"best_model_{epoch}\",  \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.07736307854495153,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.006180254073038927,\n",
    "        hidden_size=20,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.266097695477857,\n",
    "        hidden_continuous_size=10,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8335b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482b256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
