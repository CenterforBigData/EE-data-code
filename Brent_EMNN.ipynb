{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b02869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415d0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PyEMD import CEEMDAN\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "data = pd.read_excel('./Datasets/Original Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only the training data based on the 'time_idx'\n",
    "train_data = data[data['time_idx'] <= 712]\n",
    "wti_train = train_data['WTI']\n",
    "brent_train = train_data['Brent']\n",
    "\n",
    "# Initialize the CEEMDAN object with a specified number of trials for robust decomposition\n",
    "ceemdan = CEEMDAN()\n",
    "ceemdan.trials = 100\n",
    "\n",
    "# Function to decompose a time series using the CEEMDAN method\n",
    "def decompose_data(series):\n",
    "    imfs = ceemdan(series)\n",
    "     # Calculate residuals by subtracting the sum of the first two IMFs from the original series if more than two IMFs are present\n",
    "    residual = series - sum(imfs[:2]) if len(imfs) > 2 else series - sum(imfs)\n",
    "    return imfs[:2], residual\n",
    "\n",
    "# Decompose the WTI and Brent training datasets\n",
    "wti_imfs, wti_residual = decompose_data(wti_train.values)\n",
    "brent_imfs, brent_residual = decompose_data(brent_train.values)\n",
    "\n",
    "# Append decomposed components to the original dataset and adjust by adding 10 for normalization\n",
    "# The addition of 10 to the first two IMFs (Intrinsic Mode Functions) is a preprocessing step to normalize their scale.\n",
    "# Since IMFs often oscillate around zero, this shift facilitates better learning and training stability for subsequent modeling.\n",
    "# This normalization is reversed in the final analysis to maintain the original data scale.\n",
    "for i, imf in enumerate(wti_imfs):\n",
    "    data[f'WTI_IMF{i+1}'] = pd.Series(imf + 10, index=train_data.index) # Adjust WTI IMFs\n",
    "data['WTI_Res'] = pd.Series(wti_residual, index=train_data.index) # Append WTI residual\n",
    "\n",
    "for i, imf in enumerate(brent_imfs):\n",
    "    data[f'Brent_IMF{i+1}'] = pd.Series(imf + 10, index=train_data.index)\n",
    "data['Brent_Res'] = pd.Series(brent_residual, index=train_data.index)\n",
    "\n",
    "# Save the enriched dataset back to the Excel file\n",
    "data.to_excel('./Datasets/Original Data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 Extended Window Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e095a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./Datasets/CEEMDAN Decomposed Data.xlsx')\n",
    "\n",
    "# Filter data for training based on the 'time_idx'\n",
    "train_data = data[data['time_idx'] <= 712]\n",
    "N1 = 30  # Main window size for feature extraction\n",
    "N2 = 5   # Extension window size\n",
    "\n",
    "mapping_pairs = []\n",
    "for t in range(1, len(train_data) - N1 - N2 + 1):\n",
    "    W_t = train_data.iloc[t-1:t+N1-1]  # Extract the main window data for past N1 days\n",
    "    D_k_t_plus_N2 = train_data.iloc[t+N2-1:t+N1+N2-1]  # Extract the extended window data for target IMF components at N2 days ahead\n",
    "\n",
    "    W_t_data = W_t[['Brent', 'time_idx']]\n",
    "    D_k_data = D_k_t_plus_N2[['Brent_IMF1', 'Brent_IMF2', 'Brent_Res']]\n",
    "    \n",
    "    # Collect the pair of feature matrix and target matrix for training\n",
    "    mapping_pairs.append((W_t_data, D_k_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 Training the Enhancing Mapping Neural Network (EMNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217765ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define a function to build a multi-layer neural network model for regression\n",
    "def build_mnn_model(input_dim):\n",
    "    # Construct a fully connected neural network with four hidden layers and one output layer\n",
    "    model = Sequential([\n",
    "        Dense(1024, input_dim=input_dim, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dense(1)   \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse') # Compile model with MSE loss and Adam optimizer\n",
    "    return model\n",
    "\n",
    "# Extract features and targets for training from mapping pairs\n",
    "train_features = np.array([pair[0].values.flatten() for pair in mapping_pairs])\n",
    "train_targets_imf1 = np.array([pair[1]['Brent_IMF1'].values[-1] for pair in mapping_pairs]).reshape(-1, 1)\n",
    "train_targets_imf2 = np.array([pair[1]['Brent_IMF2'].values[-1] for pair in mapping_pairs]).reshape(-1, 1)\n",
    "train_targets_residual = np.array([pair[1]['Brent_Res'].values[-1] for pair in mapping_pairs]).reshape(-1, 1)\n",
    "\n",
    "# Build models for each IMF and the residual\n",
    "input_dim = W_t_data.shape[1] * W_t_data.shape[0]  \n",
    "imf1_model = build_mnn_model(input_dim)\n",
    "imf2_model = build_mnn_model(input_dim)\n",
    "residual_model = build_mnn_model(input_dim)\n",
    "\n",
    "# Configure callbacks to save the best model based on the lowest loss\n",
    "checkpoint_imf1 = ModelCheckpoint('./EEMD_saved_models/Brent_imf1_best_model.h5', monitor='loss', save_best_only=True, verbose=1)\n",
    "checkpoint_imf2 = ModelCheckpoint('./EEMD_saved_models/Brent_imf2_best_model.h5', monitor='loss', save_best_only=True, verbose=1)\n",
    "checkpoint_residual = ModelCheckpoint('./EEMD_saved_models/Brent_residual_best_model.h5', monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Conditionally train models if all data arrays match in length\n",
    "if (len(train_features) == len(train_targets_imf1) == len(train_targets_imf2) == len(train_targets_residual)):\n",
    "    imf1_model.fit(train_features, train_targets_imf1, epochs=50, batch_size=32, verbose=1, callbacks=[checkpoint_imf1])\n",
    "    imf2_model.fit(train_features, train_targets_imf2, epochs=50, batch_size=32, verbose=1, callbacks=[checkpoint_imf2])\n",
    "    residual_model.fit(train_features, train_targets_residual, epochs=50, batch_size=32, verbose=1, callbacks=[checkpoint_residual])\n",
    "else:\n",
    "    print(\"Error: Data cardinality mismatch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 Prediction/Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e89298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predictions integrated and Excel file updated.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained models\n",
    "imf1_model = load_model('./EEMD_saved_models/Brent_imf1_best_model.h5')\n",
    "imf2_model = load_model('./EEMD_saved_models/Brent_imf2_best_model.h5')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('./Datasets/CEEMDAN Decomposed Data.xlsx')\n",
    "data.sort_values('time_idx', inplace=True)\n",
    "\n",
    "# Define prediction range\n",
    "start_time_idx = 713 \n",
    "end_time_idx = data['time_idx'].max() \n",
    "\n",
    "# Predict and update DataFrame\n",
    "for time_idx in range(start_time_idx, end_time_idx + 1):\n",
    "    target_idx = time_idx - 1\n",
    "\n",
    "    if target_idx >= 0:\n",
    "        window_data = data.iloc[time_idx - 30:time_idx][['Brent', 'time_idx']].values.flatten()\n",
    "        window_data = np.expand_dims(window_data, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        predicted_imf1 = imf1_model.predict(window_data)  \n",
    "        predicted_imf2 = imf2_model.predict(window_data) \n",
    "\n",
    "        # Update DataFrame\n",
    "        data.at[target_idx, 'Brent_IMF1'] = predicted_imf1.flatten()[0]\n",
    "        data.at[target_idx, 'Brent_IMF2'] = predicted_imf2.flatten()[0]\n",
    "\n",
    "        # Calculate and adjust residual\n",
    "        if 'Brent' in data.columns:\n",
    "            predicted_residual = data.at[target_idx, 'Brent'] - (predicted_imf1.flatten()[0] + predicted_imf2.flatten()[0])\n",
    "            data.at[target_idx, 'Brent_Res'] = predicted_residual + 20  # Include +20 adjustment here\n",
    "\n",
    "# Save the updated DataFrame back to Excel\n",
    "data.to_excel('./Datasets/CEEMDAN Decomposed Data.xlsx', index=False)\n",
    "\n",
    "print(\"Predictions integrated and Excel file updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81e95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece6feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
