{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb828181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is divided into two main parts:\n",
    "\n",
    "# Part 1: Model Validation\n",
    "# In this section, the focus is on validating pre-trained models. \n",
    "# It involves assessing the performance of models that have already been trained, \n",
    "# using predefined parameters such as time index ranges for the validation dataset. \n",
    "# The goal is to evaluate the predictive accuracy of these models on test data.\n",
    "\n",
    "# Part 2: Model Training\n",
    "# This section is dedicated to training your own models. \n",
    "# It guides you through the process of setting up and configuring a new model, \n",
    "# including defining its parameters and architecture. \n",
    "# You will train the model using your dataset, allowing for experimentation \n",
    "# with different configurations to develop a model that best suits your data and prediction objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d7b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('./dataset/EMNN Decomposed Data.xlsx')\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "data[\"Brent\"] = data[\"WTI\"].astype(\"float64\")\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data[\"weekday\"] = data[\"weekday\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e151b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 256  # set this between 32 to 128\n",
    "\n",
    "\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# move dataloaders to device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76579de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82897a48",
   "metadata": {},
   "source": [
    "## Part 1: Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26b3ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5402dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for storing the trained model\n",
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=41-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v84.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v85.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=19-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=23-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v86.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v87.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v88.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v89.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=32-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v90.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v20.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v91.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v31.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=39-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v93.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=1-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v32.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v94.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v95.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v96.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v97.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v98.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v99.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v100.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v30.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v101.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v102.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v103.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v104.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=40-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v105.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v106.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v107.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=24-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v108.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v109.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v110.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v111.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v112.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v113.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v114.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v115.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v116.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v31.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v32.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v117.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v118.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v41.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v125.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v42.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v126.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=35.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v43.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=23-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v127.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v128.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v44.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v129.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v22.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v130.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=33-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v45.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v46.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v47.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=32-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v48.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v10.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c30b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Brent_IMF1\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Brent_IMF1\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9902639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "IMF1_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348431c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c9e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37a40d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94ff51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=22-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=19-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=6-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=10-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v131.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v34.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=31-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v132.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=19-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=36-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v49.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v16.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=0-v35.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v133.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=5-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=8-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v23.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v24.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v134.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=5-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=2-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v25.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=31-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v135.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=35-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v26.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=6-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v50.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v52.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=45-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=6-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v53.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=42-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=20-v1.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=22-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=20-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v17.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=20-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v16.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=7-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v17.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=2-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=20-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=43-v18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=33-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=21-v4.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v54.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=2-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=35-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v138.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=44-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=46-v27.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=18-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=23-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v15.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54193d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "805ea6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Brent_IMF2\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Brent_IMF2\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85fce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "IMF2_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d4b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea0586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "559bbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d7012be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_paths=['.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=25-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=25-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v17.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=3-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=30-v15.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v18.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v19.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=13-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=40-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v20.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=14-v2.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=19-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=15-v7.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=49-v141.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=28-v20.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=21-v5.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=14-v3.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=19-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=31-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=33-v11.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=16-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=32-v9.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=26-v10.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=4-v6.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=37-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=34-v21.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=38-v12.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=27-v13.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=2-v8.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=47-v14.ckpt', '.\\\\TFT_saved_models\\\\best_model_epoch=48-v64.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "564bec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802\n",
    "prediction_length = 1  # The step length for each prediction\n",
    "max_encoder_length=30\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_predictions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ae6a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating the validation set\n",
    "training = TimeSeriesDataSet(\n",
    "    data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Brent_Res\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Brent_Res\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c3abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for making predictions\n",
    "for i, model_path in enumerate(pretrained_model_paths):\n",
    "    start_idx = start_prediction_idx + i * prediction_length\n",
    "    end_idx = start_idx + prediction_length\n",
    "\n",
    "    if end_idx > end_prediction_idx:\n",
    "        end_idx = end_prediction_idx + 1  \n",
    "    \n",
    "    # Create a new validation dataset\n",
    "    validation_data = data[data['time_idx'] < end_idx].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load the pretrained model and make predictions\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  # Retrieve current prediction results\n",
    "    current_prediction_numpy = current_prediction.numpy().flatten()\n",
    "    all_predictions.extend(current_prediction_numpy.tolist())\n",
    "\n",
    "Res_fore=all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b8c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7d8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631523ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2777470",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [87.86, 91.37, 90.7, 87.58, 88.4, 94.33, 90.99, 92.52, 91.99, 93.12, 93.72, 91.88, 88.0, 90.14, 88.45, 90.73, 90.73, 86.82, 86.92, 89.02, 87.55, 87.31, 83.43, 81.46, 81.74, 83.66, 84.09, 84.2, 82.4, 77.73, 81.22, 83.25, 82.42, 81.76, 80.85, 79.82, 79.49, 81.66, 82.98, 81.72, 78.72, 78.16, 77.27, 74.33, 74.21, 75.94, 75.75, 74.11, 74.14, 77.05, 76.84, 78.89, 79.82, 81.1, 80.73, 80.23, 80.6, 80.97, 79.04, 77.69, 76.24, 77.18, 75.79, 78.31, 75.47, 77.97, 78.46, 80.21, 79.89, 79.76, 80.15, 78.88, 81.04, 80.71, 81.7, 82.04, 82.15, 82.33, 83.34, 83.99, 84.14, 82.98, 82.2, 79.54, 79.3, 80.46, 81.18, 83.01, 83.58, 83.88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97eb8dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7199421106974282\n",
      "RMSE: 1.0528983383515984\n",
      "MAPE: 0.008829412043259627\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine IMF1, IMF2, and residual components to get the final forecast\n",
    "final = [sum(x) for x in zip(IMF1_fore, IMF2_fore, Res_fore)]\n",
    "# Adjust the elements in the 'final' list to balance the previous increments of 10 each to IMF1 and IMF2, \n",
    "# effectively removing the added 20 units per element.\n",
    "final = [x - 20 for x in final]\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE)\n",
    "mae = np.mean(np.abs(np.array(true_values) - np.array(final)))\n",
    "rmse = np.sqrt(np.mean(np.square(np.array(true_values) - np.array(final))))\n",
    "mape = np.mean(np.abs(np.array(true_values) - np.array(final)) / np.array(true_values))\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be7357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761e077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d8b1bc",
   "metadata": {},
   "source": [
    "## Part 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training the Temporal Fusion Transformer (TFT) model, \n",
    "# it is beneficial to employ the Tree-structured Parzen Estimator (TPE) method for hyperparameter optimization.\n",
    "# This step aids in selecting the optimal hyperparameters for the TFT model. \n",
    "# However, the TPE method can be computationally intensive and involves randomness, \n",
    "# hence it is optional. We have already incorporated the optimal hyperparameters\n",
    "# obtained from running this method into the corresponding TFT model.\n",
    "# If you wish to perform this optimization and discover your own optimal hyperparameters,\n",
    "# you may run the following code. Note that this process may significantly impact computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_prediction_length = 1\n",
    "# max_encoder_length = 30\n",
    "\n",
    "# training = TimeSeriesDataSet(\n",
    "#     data[(data.time_idx >= 1) & (data.time_idx <= 712)],\n",
    "#     time_idx=\"time_idx\",\n",
    "#     target=\"Brent_IMF1\",\n",
    "#     min_encoder_length=max_encoder_length // 2, \n",
    "#     max_encoder_length=max_encoder_length,\n",
    "#     min_prediction_length=1,\n",
    "#     max_prediction_length=max_prediction_length,\n",
    "#     time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "#     time_varying_known_reals=[\"time_idx\"],\n",
    "#     time_varying_unknown_categoricals=[],\n",
    "#     time_varying_unknown_reals=[\n",
    "#         \"Brent_IMF1\",\n",
    "        \n",
    "#     ],\n",
    "#     group_ids=['destination'],\n",
    "#     target_normalizer=GroupNormalizer(\n",
    "#         groups=['destination'], transformation=\"softplus\"),\n",
    "#     add_relative_time_idx=True,\n",
    "#     add_target_scales=True,\n",
    "#     add_encoder_length=True,\n",
    "#     allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    "# )\n",
    "# validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "\n",
    "# # create dataloaders for model\n",
    "# batch_size = 256  # set this between 32 to 128\n",
    "\n",
    "\n",
    "# # check if GPU is available\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "# # move dataloaders to device\n",
    "\n",
    "\n",
    "# train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "# val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10 , num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c232620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# # create study\n",
    "# study = optimize_hyperparameters(\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     model_path=\"optuna_test\",\n",
    "#     n_trials=50,\n",
    "#     max_epochs=50,\n",
    "#     gradient_clip_val_range=(0.01, 1.0),\n",
    "#     hidden_size_range=(8, 128),\n",
    "#     hidden_continuous_size_range=(8, 128),\n",
    "#     attention_head_size_range=(1, 4),\n",
    "#     learning_rate_range=(0.001, 0.1),\n",
    "#     dropout_range=(0.1, 0.3),\n",
    "#     trainer_kwargs=dict(limit_train_batches=30),\n",
    "#     reduce_on_plateau_patience=4,\n",
    "#     use_learning_rate_finder=False\n",
    "# )\n",
    "\n",
    "\n",
    "# # save study results - also we can resume tuning at a later point in time\n",
    "# with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "#     pickle.dump(study, fout)\n",
    "\n",
    "# # show best hyperparameters\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec3f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f228a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 1\n",
    "\n",
    "# Define the time index range for the validation dataset\n",
    "start_prediction_idx = 713 # Starting index for prediction\n",
    "end_prediction_idx = 802 # Ending index for prediction\n",
    "\n",
    "\n",
    "# Set the prediction length to 1, indicating that the model predicts 1 steps ahead\n",
    "prediction_length = 1\n",
    "\n",
    "# Note: The test set comprises 90 time points. The prediction_length of 1 means that \n",
    "# the models trained in this session are designed to forecast 1 steps ahead. The \n",
    "# start and end indices (713 to 802) specify the time points covered by the \n",
    "# model's predictions. These indices also determine the number of models to be trained, \n",
    "# calculated as: (end_prediction_idx - start_prediction_idx + 1) / prediction_length.\n",
    "# For start_prediction_idx = 713 and end_prediction_idx = 802 with prediction_length = 1,\n",
    "# the total number of models trained in the loop is 90.\n",
    "\n",
    "max_encoder_length = 30 \n",
    "\n",
    "# Initialize an empty list to store all prediction values\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize an empty list to store the best models for each prediction\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b758d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: The models are resource-intensive, requiring a high-performance GPU. Training \n",
    "# too many models in a single loop may lead to exhaustion of computer resources, causing \n",
    "# the program to crash. In such cases, you may need to restart the program or adjust the \n",
    "# start and end indices to reduce the number of models trained in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40532186",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1],  \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"Brent_IMF1\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"Brent_IMF1\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    \n",
    "    # configure network and trainer\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,  \n",
    "        filename=\"best_model_{epoch}\", \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.12606822464127176,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.0037662293387837106,\n",
    "        hidden_size=8,\n",
    "        attention_head_size=1,\n",
    "        dropout=0.11405196829177033,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef9361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117b2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f519e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802 \n",
    "\n",
    "prediction_length = 1\n",
    "max_encoder_length = 30 \n",
    "\n",
    "all_predictions = []\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1],  \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"Brent_IMF2\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"Brent_IMF2\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1,  \n",
    "        filename=\"best_model_{epoch}\",  \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.015731346875597366,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.08854054714306236,\n",
    "        hidden_size=41,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.25541380275080905,\n",
    "        hidden_continuous_size=14,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5537e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470a81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47067bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b996ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prediction_idx = 713\n",
    "end_prediction_idx = 802 \n",
    "\n",
    "prediction_length = 1\n",
    "max_encoder_length = 30 \n",
    "\n",
    "all_predictions = []\n",
    "all_best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_idx in range(start_prediction_idx, end_prediction_idx + 1, prediction_length):\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= prediction_idx - 1],  \n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"Brent_Res\",\n",
    "        min_encoder_length=max_encoder_length // 2, \n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=prediction_length,\n",
    "        time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\n",
    "            \"Brent_Res\",\n",
    "\n",
    "        ],\n",
    "        group_ids=['destination'],\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['destination'], transformation=\"softplus\"),\n",
    "\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    validation_data = data[lambda x: x.time_idx <= prediction_idx - 1 + prediction_length].copy()\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, validation_data, predict=True, stop_randomization=True)\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "        save_top_k=1, \n",
    "        filename=\"best_model_{epoch}\",  \n",
    "        dirpath=\"saved_models\"\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        gpus=1,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.4804651992814277,\n",
    "        limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.05225339638821306,\n",
    "        hidden_size=37,\n",
    "        attention_head_size=1,\n",
    "        dropout=0.2747736782762395,\n",
    "        hidden_continuous_size=20,\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "\n",
    "\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    current_model=best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "    all_best_models.append(current_model)\n",
    "    current_prediction = raw_predictions[0][:, :, 3]  \n",
    "    all_predictions.append(current_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684e728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
